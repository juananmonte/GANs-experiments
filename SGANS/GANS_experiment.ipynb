{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dfe9412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:17.834287Z",
     "start_time": "2021-07-16T15:30:15.662206Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlp\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "from numpy import savez_compressed\n",
    "from numpy import load \n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img \n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dropout \n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.applications import VGG16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc03d38",
   "metadata": {},
   "source": [
    "# Load .npz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6db9c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:17.850200Z",
     "start_time": "2021-07-16T15:30:17.834287Z"
    }
   },
   "outputs": [],
   "source": [
    "# #### combining the npz\n",
    "npz_path = \"E:/UCF_Crimes/npz_120/\"\n",
    "npz_list = os.listdir(npz_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbad8091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:17.866201Z",
     "start_time": "2021-07-16T15:30:17.854203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crimes_120_0.npz',\n",
       " 'crimes_120_1.npz',\n",
       " 'crimes_120_10.npz',\n",
       " 'crimes_120_11.npz',\n",
       " 'crimes_120_12.npz',\n",
       " 'crimes_120_2.npz',\n",
       " 'crimes_120_3.npz',\n",
       " 'crimes_120_4.npz',\n",
       " 'crimes_120_5.npz',\n",
       " 'crimes_120_6.npz',\n",
       " 'crimes_120_7.npz',\n",
       " 'crimes_120_8.npz',\n",
       " 'crimes_120_9.npz']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc00b487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:17.882200Z",
     "start_time": "2021-07-16T15:30:17.866201Z"
    }
   },
   "outputs": [],
   "source": [
    "full_npz = itemgetter(0,1,3,4)(npz_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be907454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:17.898211Z",
     "start_time": "2021-07-16T15:30:17.882200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:/UCF_Crimes/npz_120/crimes_120_0.npz',\n",
       " 'E:/UCF_Crimes/npz_120/crimes_120_1.npz',\n",
       " 'E:/UCF_Crimes/npz_120/crimes_120_11.npz',\n",
       " 'E:/UCF_Crimes/npz_120/crimes_120_12.npz']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_npz = [npz_path+npz for npz in full_npz]\n",
    "full_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3822de26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:17.914204Z",
     "start_time": "2021-07-16T15:30:17.902200Z"
    }
   },
   "outputs": [],
   "source": [
    "data_all = [np.load(fname) for fname in full_npz] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f028f8a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.302272Z",
     "start_time": "2021-07-16T15:30:17.914204Z"
    }
   },
   "outputs": [],
   "source": [
    "images = list()\n",
    "images += [npz['arr_0'] for npz in data_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8822733b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.318316Z",
     "start_time": "2021-07-16T15:30:42.306249Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = list()\n",
    "labels += [npz['arr_1'] for npz in data_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21f389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fff98fc6",
   "metadata": {},
   "source": [
    "# Before all, define a custom activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e45435f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.334269Z",
     "start_time": "2021-07-16T15:30:42.322254Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom activation function\n",
    "def custom_activation(output):\n",
    "    logexpsum = backend.sum(backend.exp(output), axis=-1, keepdims=True)\n",
    "    result = logexpsum / (logexpsum + 1.0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440c820",
   "metadata": {},
   "source": [
    "# Defining the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c987e",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a032487a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.350248Z",
     "start_time": "2021-07-16T15:30:42.334269Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the standalone supervised and unsupervised discriminator models\n",
    "def define_discriminator(in_shape=(120, 120, 3), n_classes=14):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "# image input\n",
    "    in_image = Input(shape=in_shape)\n",
    "#downsample\n",
    "    fe = Conv2D(32, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
    "    fe = BatchNormalization(axis=1)(fe)\n",
    "    fe = LeakyReLU(alpha=0.4)(fe)\n",
    "# # downsample\n",
    "    fe = Conv2D(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "    fe = BatchNormalization(axis=1)(fe)\n",
    "    fe = LeakyReLU(alpha=0.4)(fe)\n",
    "# #downsample\n",
    "    fe = Conv2D(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "    fe = BatchNormalization(axis=1)(fe)\n",
    "    fe = LeakyReLU(alpha=0.4)(fe)\n",
    "# downsample\n",
    "    fe = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(fe)\n",
    "    fe = BatchNormalization(axis=1)(fe)\n",
    "    fe = LeakyReLU(alpha=0.4)(fe)\n",
    "    \n",
    "# flatten feature maps\n",
    "    fe = Flatten()(fe)\n",
    "# dropout\n",
    "    fe = Dropout(0.5)(fe)\n",
    "# output layer nodes\n",
    "    fe = Dense(n_classes)(fe)#kernel_initializer=init)(fe)\n",
    "# supervised output\n",
    "    c_out_layer = Activation('softmax')(fe)\n",
    "# define and compile supervised discriminator model\n",
    "    c_model = Model(in_image, c_out_layer)\n",
    "    c_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.006, beta_1=0.5), metrics=['accuracy'])\n",
    "# unsupervised output\n",
    "    d_out_layer = Lambda(custom_activation)(fe)\n",
    "# define and compile unsupervised discriminator model\n",
    "    d_model = Model(in_image, d_out_layer)\n",
    "    d_model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "    return d_model, c_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9265c6",
   "metadata": {},
   "source": [
    "##  Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67e0fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.366398Z",
     "start_time": "2021-07-16T15:30:42.350248Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    #image generator input\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "# foundation for 120x120 image (the 15*15 is the 1/8 of the size of the image. metric given by the book)\n",
    "    n_nodes = 128 * 15 * 15\n",
    "    gen = Dense(n_nodes)(in_lat)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Reshape((15, 15, 128))(gen)\n",
    "    # upsample to 30x30\n",
    "    gen = Conv2DTranspose(32, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = BatchNormalization(axis=1)(gen)\n",
    "    gen = LeakyReLU(alpha=0.4)(gen)\n",
    "#upsample to 60x60\n",
    "    gen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = BatchNormalization(axis=1)(gen)\n",
    "    gen = LeakyReLU(alpha=0.4)(gen)\n",
    "# #upsample to 120x120\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
    "    gen = BatchNormalization(axis=1)(gen)\n",
    "    gen = LeakyReLU(alpha=0.4)(gen)\n",
    "# output\n",
    "    out_layer = Conv2D(3, (15, 15), activation='tanh', padding='same', kernel_initializer=init)(gen)\n",
    "# define model \n",
    "    model = Model(in_lat, out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265187a",
   "metadata": {},
   "source": [
    "### combined generator and discriminator model (for updating the generator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0eb34d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.382304Z",
     "start_time": "2021-07-16T15:30:42.366398Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "# make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "# connect image output from generator as input to discriminator\n",
    "    gan_output = d_model(g_model.output)\n",
    "# define gan model as taking noise and outputting a classification\n",
    "    model = Model(g_model.input, gan_output)\n",
    "# compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8305aae",
   "metadata": {},
   "source": [
    "# select a supervised subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "168c95f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.398304Z",
     "start_time": "2021-07-16T15:30:42.382304Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_supervised_samples(dataset, n_samples=1200, n_classes=14):\n",
    "    X, y = dataset\n",
    "    X_list, y_list = list(), list()\n",
    "    n_per_class = int(n_samples / n_classes)\n",
    "    for n in range (0, 4):\n",
    "        for i in range(n_classes):\n",
    "            # get all images for this class\n",
    "            X_with_class = [X[n]][[y[n]] == i]\n",
    "            # choose random instances\n",
    "            ix = randint(0, len(X_with_class), n_per_class)\n",
    "            # add to list\n",
    "            [X_list.append(X_with_class[j]) for j in ix]\n",
    "            [y_list.append(i) for j in ix]\n",
    "    return asarray(X_list), asarray(y_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0876b035",
   "metadata": {},
   "source": [
    "# Select random samples from the supervised dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a42b10da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.414253Z",
     "start_time": "2021-07-16T15:30:42.398304Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_real_samples(X, y, n_samples):\n",
    "# split into images and labels\n",
    "    images = X\n",
    "    labels = y\n",
    "# choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "# select images and labels\n",
    "    X, labels = images[ix], labels[ix]\n",
    "# generate class labels\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47bc633b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.430278Z",
     "start_time": "2021-07-16T15:30:42.414253Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_real_samples2(x, y, n_samples):\n",
    "# split into images and labels\n",
    "    ix = randint(0, x[0].shape[0], n_samples)\n",
    "    im_list = list()\n",
    "    lab_list = list()\n",
    "# select images and labels\n",
    "    for n in range (0, 4): \n",
    "        for i in x[n][ix]:\n",
    "            im_list.append(i)\n",
    "        for c in y[n][ix]:\n",
    "            lab_list.append(c)\n",
    "# generate class labels\n",
    "    y = ones((n_samples*4, 1))\n",
    "    return [asarray(im_list), asarray(lab_list)], y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2104c3",
   "metadata": {},
   "source": [
    "## generate points in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ea4eb9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.446249Z",
     "start_time": "2021-07-16T15:30:42.430278Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tz_input = randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tz_input = z_input.reshape(n_samples, latent_dim)\n",
    "\treturn z_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e70a5b",
   "metadata": {},
   "source": [
    "## Generate the fake images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7041bc2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.462254Z",
     "start_time": "2021-07-16T15:30:42.446249Z"
    }
   },
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "# generate points in latent space\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "# predict outputs\n",
    "    images = generator.predict(z_input)\n",
    "# create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return images, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12061a",
   "metadata": {},
   "source": [
    "# save as a plot and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1cee53e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.478254Z",
     "start_time": "2021-07-16T15:30:42.462254Z"
    }
   },
   "outputs": [],
   "source": [
    "def summarize_performance(step, g_model, c_model, latent_dim, dataset, n_samples=100):\n",
    "# prepare fake examples\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "# scale from [-1,1] to [0,1]\n",
    "    X = (X + 1) / 2.0\n",
    "# plot images\n",
    "    for i in range(100):\n",
    "# define subplot\n",
    "        plt.subplot(10, 10, i+1)\n",
    "# turn off axis\n",
    "        plt.axis('off')\n",
    "    # plot raw pixel data\n",
    "        plt.imshow(X[i, :, :, 0])#, cmap='gray_r')\n",
    "# save plot to file\n",
    "    filename1 = 'E:/UCF_Crimes/models results/generated_plot_%04d.png' % (step+1)\n",
    "    plt.savefig(filename1)\n",
    "    plt.close()\n",
    "# evaluate the classifier model\n",
    "    X, y = dataset\n",
    "    \n",
    "    for n in range(0, 4):\n",
    "        _, acc = c_model.evaluate(X[n], y[n], verbose=1)\n",
    "        print('Classifier Accuracy: %.3f%%' % (acc * 100))\n",
    "# save the generator model\n",
    "    filename2 = 'E:/UCF_Crimes/models results/g_model_%04d.h5' % (step+1) \n",
    "    g_model.save(filename2)\n",
    "# save the classifier model\n",
    "    filename3 = 'E:/UCF_Crimes/models results/c_model_%04d_%01d.h5' % (step+1, (acc *100))\n",
    "    c_model.save(filename3)\n",
    "    print('>Saved: %s, %s, and %s' % (filename1, filename2, filename3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a5ac8",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00b84651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.494255Z",
     "start_time": "2021-07-16T15:30:42.478254Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(g_model, d_model, c_model, gan_model, dataset, latent_dim, n_epochs=50, n_batch=50):\n",
    "# select supervised dataset\n",
    "    X_sup, y_sup = select_supervised_samples(dataset)\n",
    "    print(X_sup.shape, y_sup.shape)\n",
    "# calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(30)\n",
    "# calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "# calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    \n",
    "    print('n_epochs=%d, n_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (n_epochs, n_batch, half_batch, bat_per_epo, n_steps))\n",
    "    \n",
    "# manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "# update supervised discriminator (c)\n",
    "        [Xsup_real, ysup_real], _ = generate_real_samples(X_sup, y_sup, half_batch)\n",
    "        c_loss, c_acc = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "        \n",
    "# update unsupervised discriminator (d)\n",
    "        [X_real, y_real], _ = generate_real_samples2(images, labels, half_batch)\n",
    "        d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "        \n",
    "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "        \n",
    "# update generator (g)\n",
    "        X_gan, y_gan = generate_latent_points(latent_dim, n_batch), ones((n_batch, 1))\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        \n",
    "# summarize loss on this batch\n",
    "        print('>%d, c[%.3f,%.0f], d[%.3f,%.3f], g[%.5f]' % (i+1, c_loss, c_acc*100, d_loss1, d_loss2, g_loss))\n",
    "# evaluate the model performance every so often\n",
    "        if (i+1) % (bat_per_epo * 1) == 0:\n",
    "            summarize_performance(i, g_model, c_model, latent_dim, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef157222",
   "metadata": {},
   "source": [
    "## size of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "077bd7c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:42.510253Z",
     "start_time": "2021-07-16T15:30:42.494255Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513ff6e",
   "metadata": {},
   "source": [
    "##  create the discriminator models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d962bdb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:44.937296Z",
     "start_time": "2021-07-16T15:30:42.514268Z"
    }
   },
   "outputs": [],
   "source": [
    "d_model, c_model = define_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2169d64",
   "metadata": {},
   "source": [
    "## create the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7256b70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:45.109198Z",
     "start_time": "2021-07-16T15:30:44.937296Z"
    }
   },
   "outputs": [],
   "source": [
    "g_model = define_generator(latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d4a93",
   "metadata": {},
   "source": [
    "## Create the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7277609",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:45.201292Z",
     "start_time": "2021-07-16T15:30:45.109198Z"
    }
   },
   "outputs": [],
   "source": [
    "gan_model = define_gan(g_model, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c90d4",
   "metadata": {},
   "source": [
    "## Assigning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2fed85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:30:45.217233Z",
     "start_time": "2021-07-16T15:30:45.205196Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652a667",
   "metadata": {},
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00dc0423",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-16T15:36:05.186878Z",
     "start_time": "2021-07-16T15:30:45.221206Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4760, 120, 120, 3) (4760,)\n",
      "n_epochs=50, n_batch=50, 1/2=25, b/e=30, steps=1500\n",
      ">1, c[2.937,24], d[77.829,0.998], g[0.00460]\n",
      ">2, c[10.731,12], d[58.226,1.000], g[0.00453]\n",
      ">3, c[7.925,0], d[63.840,1.000], g[0.00000]\n",
      ">4, c[14.790,8], d[64.349,1.000], g[0.00000]\n",
      ">5, c[8.959,12], d[54.334,1.000], g[nan]\n",
      ">6, c[9.247,8], d[55.094,nan], g[nan]\n",
      ">7, c[nan,16], d[nan,nan], g[nan]\n",
      ">8, c[nan,4], d[nan,nan], g[nan]\n",
      ">9, c[nan,4], d[nan,nan], g[nan]\n",
      ">10, c[nan,16], d[nan,nan], g[nan]\n",
      ">11, c[nan,4], d[nan,nan], g[nan]\n",
      ">12, c[nan,4], d[nan,nan], g[nan]\n",
      ">13, c[nan,8], d[nan,nan], g[nan]\n",
      ">14, c[nan,0], d[nan,nan], g[nan]\n",
      ">15, c[nan,8], d[nan,nan], g[nan]\n",
      ">16, c[nan,12], d[nan,nan], g[nan]\n",
      ">17, c[nan,8], d[nan,nan], g[nan]\n",
      ">18, c[nan,8], d[nan,nan], g[nan]\n",
      ">19, c[nan,0], d[nan,nan], g[nan]\n",
      ">20, c[nan,16], d[nan,nan], g[nan]\n",
      ">21, c[nan,16], d[nan,nan], g[nan]\n",
      ">22, c[nan,12], d[nan,nan], g[nan]\n",
      ">23, c[nan,12], d[nan,nan], g[nan]\n",
      ">24, c[nan,12], d[nan,nan], g[nan]\n",
      ">25, c[nan,8], d[nan,nan], g[nan]\n",
      ">26, c[nan,12], d[nan,nan], g[nan]\n",
      ">27, c[nan,0], d[nan,nan], g[nan]\n",
      ">28, c[nan,4], d[nan,nan], g[nan]\n",
      ">29, c[nan,4], d[nan,nan], g[nan]\n",
      ">30, c[nan,8], d[nan,nan], g[nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\matplotlib\\image.py:443: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = np.float64(self.norm.vmax) - np.float64(self.norm.vmin)\n",
      "C:\\Users\\MSI\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\matplotlib\\image.py:444: UserWarning: Warning: converting a masked element to nan.\n",
      "  vmid = np.float64(self.norm.vmin) + dv / 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 7s 20ms/step - loss: nan - accuracy: 0.0000e+00: 1s - loss: nan - accuracy: 0.00 - ETA: 1s - loss: nan \n",
      "Classifier Accuracy: 0.000%\n",
      "313/313 [==============================] - 6s 20ms/step - loss: nan - accuracy: 0.0000e+00: 2s\n",
      "Classifier Accuracy: 0.000%\n",
      "313/313 [==============================] - 6s 20ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Classifier Accuracy: 0.000%\n",
      "313/313 [==============================] - 6s 20ms/step - loss: nan - accuracy: 0.0000e+00\n",
      "Classifier Accuracy: 0.000%\n",
      ">Saved: E:/UCF_Crimes/models results/generated_plot_0030.png, E:/UCF_Crimes/models results/g_model_0030.h5, and E:/UCF_Crimes/models results/c_model_0030_0.h5\n",
      ">31, c[nan,0], d[nan,nan], g[nan]\n",
      ">32, c[nan,8], d[nan,nan], g[nan]\n",
      ">33, c[nan,8], d[nan,nan], g[nan]\n",
      ">34, c[nan,8], d[nan,nan], g[nan]\n",
      ">35, c[nan,0], d[nan,nan], g[nan]\n",
      ">36, c[nan,12], d[nan,nan], g[nan]\n",
      ">37, c[nan,4], d[nan,nan], g[nan]\n",
      ">38, c[nan,4], d[nan,nan], g[nan]\n",
      ">39, c[nan,12], d[nan,nan], g[nan]\n",
      ">40, c[nan,0], d[nan,nan], g[nan]\n",
      ">41, c[nan,0], d[nan,nan], g[nan]\n",
      ">42, c[nan,4], d[nan,nan], g[nan]\n",
      ">43, c[nan,12], d[nan,nan], g[nan]\n",
      ">44, c[nan,0], d[nan,nan], g[nan]\n",
      ">45, c[nan,4], d[nan,nan], g[nan]\n",
      ">46, c[nan,4], d[nan,nan], g[nan]\n",
      ">47, c[nan,8], d[nan,nan], g[nan]\n",
      ">48, c[nan,4], d[nan,nan], g[nan]\n",
      ">49, c[nan,16], d[nan,nan], g[nan]\n",
      ">50, c[nan,12], d[nan,nan], g[nan]\n",
      ">51, c[nan,4], d[nan,nan], g[nan]\n",
      ">52, c[nan,0], d[nan,nan], g[nan]\n",
      ">53, c[nan,12], d[nan,nan], g[nan]\n",
      ">54, c[nan,8], d[nan,nan], g[nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-df2bd3896ad2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-11af4991ab0b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(g_model, d_model, c_model, gan_model, dataset, latent_dim, n_epochs, n_batch)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# update generator (g)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mX_gan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_gan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_latent_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_gan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_gan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# summarize loss on this batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1730\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1731\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1732\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1667\u001b[0m     \"\"\"\n\u001b[0;32m   1668\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m       \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m   def train_on_batch(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m--> 253\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3704\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3705\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3706\u001b[1;33m       \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3707\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3708\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    890\u001b[0m              \"shape %s are incompatible\") %\n\u001b[0;32m    891\u001b[0m             (tensor_name, self._shape, value_tensor.shape))\n\u001b[1;32m--> 892\u001b[1;33m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0m\u001b[0;32m    893\u001b[0m           self.handle, value_tensor, name=name)\n\u001b[0;32m    894\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-latest\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    138\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    141\u001b[0m         _ctx, \"AssignVariableOp\", name, resource, value)\n\u001b[0;32m    142\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(g_model, d_model, c_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384dd04a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tf-latest)",
   "language": "python",
   "name": "tf-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
